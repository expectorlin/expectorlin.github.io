<!DOCTYPE html>
<head>
    <title>Bingqian Lin</title>
    <meta name="author" content="Bingqian Lin">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="Bingqian Lin">
	<meta property="og:description" content="Postdoctoral Fellow, Shanghai Jiao Tong University">
    <meta property="og:image" content="https://expectorlin.github.io/files/me-cut-new.png">
	<meta property="og:url" content="https://expectorlin.github.io/">
<!-- 	<meta name="twitter:card" content="summary_large_image"> -->
    <link rel="apple-touch-icon" href="files/ucsd-logo.png">
    <link rel="icon" type="image/png" href="files/ucsd-logo.png">
    <link rel="manifest" href="files/site.webmanifest">
    <link rel="stylesheet" href="style.css">
</head>

<div class="header noselect">
    <div class="content row">
        <div class="header-profile-picture"></div>
        <div class="header-text">
            <div class="header-name">
                <h1>Bingqian Lin</h1>
            </div>
            <div class="header-subtitle">
                Postdoctoral Fellow, Shanghai Jiao Tong University
            </div>
            <div class="header-links">
                <a class="btn" href="#contact">Email</a> /
                <a class="btn" href="https://scholar.google.com/citations?user=7tNbAJcAAAAJ&hl=zh-CN&view_op=list_works&sortby=pubdate">Google Scholar</a> /
                <a class="btn" href="https://github.com/expectorlin">GitHub</a> /
		<a class="btn" href="files/Resume_en.pdf">CV</a>
            </div>
        </div>
    </div>
</div>
<style>
    .section-spacing {
        margin-bottom: 10px; /* Adjusts space between sections */
    }

    .list-item-spacing {
        margin-bottom: 5px; /* Adjusts space between list items */
    }
</style>
<div class="content" style="padding-bottom: 64px;">
    <div>
        <p>
            I am currently a postdoctoral fellow at the <a href="https://www.sjtu.edu.cn/">Shanghai Jiao Tong University</a>, working with Prof. <a href="https://www.mvig.org/">Cewu Lu</a>. I received my Ph.D. degree in Electronics Information Engineering at <a href="https://www.sysu.edu.cn/">Sun Yat-sen University</a>, advised by Prof. <a href="https://lemondan.github.io/">Xiaodan Liang</a> and Prof. <a href="http://www.linliang.net/">Liang Lin</a> at the <a href="https://www.sysu-hcp.net/">Human Cyber Physical Intelligence Integration Lab (HCP-I2 Lab)</a>. I was a research intern at Huawei Noahâ€™s Ark Laboratory, advised by Prof. <a href="https://people.ucas.ac.cn/~jzliu?language=en">Jianzhuang Liu</a>.
            I received my BS and MS degrees from the University of Electronic Science and Technology of China (<a href="https://www.uestc.edu.cn/">UESTC</a>) and the Xiamen University (<a href="https://www.xmu.edu.cn/">XMU</a>).<br/><br/>
            My research interest lies in <span class="bold">Multi-modal understanding</span>, <span class="bold">Embodied AI</span>, and <span class="bold">Computer vision</span>. Currently, I am focusing on building generalist embodied agents driven by foundation models and the world model theory.
        </p>
    </div>
    <div class="section-spacing">
	<div>
	    <h2 class="noselect">Recent Activities</h2>
	    <ul>
		<li class="list-item-spacing" ><span class="bold">[06/2024]</span> &nbsp; Excited to receive Honors Graduate from Sun Yat-sen University!</li>
		<li class="list-item-spacing" ><span class="bold">[05/2024]</span> &nbsp; One paper on Correctable Landmark Discovery for the VLN task is accepted by <a href="https://arxiv.org/abs/2405.18721">IEEE TPAMI</a>.</li>
		<li class="list-item-spacing" ><span class="bold">[05/2024]</span> &nbsp; One paper on Map-oriented Prompting for the VLN task is accepted by <a href="https://arxiv.org/abs/2401.07314">ACL 2024</a>.</li>
		<li class="list-item-spacing" ><span class="bold">[05/2024]</span> &nbsp; I successfully defended my Ph.D. dissertation!</li>
		<li class="list-item-spacing" ><span class="bold">[03/2024]</span> &nbsp; One paper on Navigational Chain-of-Thought for the VLN task is available at <a href="https://arxiv.org/abs/2403.07376">Preprint</a>.</li>
		<li class="list-item-spacing" ><span class="bold">[04/2023]</span> &nbsp; One paper on Knowledge-Enhanced Medical Multimodal Pretraining is available at <a href="https://arxiv.org/abs/2304.14204">Preprint</a>.</li>
		<li class="list-item-spacing" ><span class="bold">[04/2023]</span> &nbsp; One paper on Training Deviation-robust Agents for the VLN task is available at <a href="https://arxiv.org/abs/2403.05770">TPAMI 2023</a>.</li>
		<li class="list-item-spacing" ><span class="bold">[03/2023]</span> &nbsp; One paper on Dynamic Graph Enhanced Contrastive Learning for the MRG task is available at <a href="https://arxiv.org/abs/2303.10323">CVPR 2023</a>.</li>
		<li class="list-item-spacing" ><span class="bold">[02/2023]</span> &nbsp; One paper on Actional Atomic-Concept Learning for the VLN task is available at <a href="https://arxiv.org/abs/2302.06072">AAAI 2023</a>.</li>
		<li class="list-item-spacing" ><span class="bold">[03/2022]</span> &nbsp; One paper on Modality-Aligned Action Prompts for the VLN task is available at <a href="https://arxiv.org/abs/2205.15509">CVPR 2022</a>.</li>
		    
		<!-- Add more list items here if needed -->
	    </ul>
	</div>
    </div>
    <div>
        <h2 class="noselect">Selected Publications/Preprints</h2>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/console.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2405.18721">Correctable Landmark Discovery via Large Models for Vision-Language Navigation</a><br/>
                <span class="bold">Bingqian Lin</span>*, Yunshuang Nie*, Ziming Wei, Yi Zhu, Hang Xu, Shikui Ma, Jianzhuang Liu, Xiaodan Liang<br/>
                <span class="italic">TPAMI</span>, 2024<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2405.18721">arXiv</a> / <a class="btn" href="https://github.com/expectorlin/CONSOLE">code</a> / <a class="btn" href="https://scholar.googleusercontent.com/scholar.bib?q=info:aawz8As7-isJ:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuDsYU:AFWwaeYAAAAAZn-FqYWKr6syabehBLI4bnvP60M&scisig=AFWwaeYAAAAAZn-FqdvwYHbg-gzrdzg3XjfXGqY&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a> 
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/navcot.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2403.07376">NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning</a><br/>
                <span class="bold">Bingqian Lin</span>*, Yunshuang Nie*, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang<br/>
                <span class="italic">arXiv</span>, 2024<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2403.07376">arXiv</a> / <a class="btn" href="https://github.com/expectorlin/NavCoT">code</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:4eMBD5Yhe-kJ:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuBElg:AFWwaeYAAAAAZn-HCli5EWPUUsvz76wDLMbWjvw&scisig=AFWwaeYAAAAAZn-HClJR8L5hT8rfLMtHijkmTAw&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/mapgpt.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2401.07314">MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation</a><br/>
                Jiaqi Chen, <span class="bold">Bingqian Lin</span>, Ran Xu, Zhenhua Chai, Xiaodan Liang, Kwan-Yee K. Wong<br/>
                <span class="italic">ACL</span>, 2024<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2401.07314">arXiv</a> / <a class="btn" href="https://github.com/chen-judge/MapGPT">code</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:nQ7JSWn7GecJ:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuOGAk:AFWwaeYAAAAAZn-IAAmazTkgcw8AyPSB6A2lwiw&scisig=AFWwaeYAAAAAZn-IALbSxLp8hJW3jMy4AkGj_xs&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/proper.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2403.05770">Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning</a><br/>
                <span class="bold">Bingqian Lin</span>*, Yanxin Long*, Yi Zhu, Fengda Zhu, Xiaodan Liang, Qixiang Ye, Liang Lin<br/>
                <span class="italic">TPAMI</span>, 2023<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2403.05770">arXiv</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:7nR7E7Fqx44J:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuPn1I:AFWwaeYAAAAAZn-Jh1JrZAYs_IO67HjdtpFQTrE&scisig=AFWwaeYAAAAAZn-Jh3nD5sEa5EtLaAZohzQVY64&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
            </div>
        </div>
	<div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/motor.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2304.14204">Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining</a><br/>
                <span class="bold">Bingqian Lin</span>*, Zicong Chen*, Mingjie Li*, Haokun Lin, Hang Xu, Yi Zhu, Jianzhuang Liu, Wenjia Cai, Lei Yang, Shen Zhao, Chenfei Wu, Ling Chen, Xiaojun Chang, Yi Yang, Lei Xing, Xiaodan Liang<br/>
                <span class="italic">arXiv</span>, 2023<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2304.14204">arXiv</a> / <a class="btn" href="https://github.com/chenzcv7/MOTOR">code</a> /  <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:QAtvH-V8wYAJ:scholar.google.com/&output=citation&scisdr=ClFhYC8oEMfL2CyjcGE:AFWwaeYAAAAAZoilaGFm6Kczd6AVG-OXuOJ6lsI&scisig=AFWwaeYAAAAAZoilaFUnl0gdBzWsKOUyIM58l9A&scisf=4&ct=citation&cd=-1&hl=zh-CN&scfhb=1">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/DCL.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2303.10323">Dynamic Graph Enhanced Contrastive Learning for Chest X-ray Report Generation</a><br/>
                Mingjie Li, <span class="bold">Bingqian Lin</span>, Zicong Chen, Haokun Lin, Xiaodan Liang, Xiaojun Chang<br/>
                <span class="italic">CVPR</span>, 2023<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2303.10323">arXiv</a> / <a class="btn" href="https://github.com/mlii0117/DCL">code</a> /  <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:1Wc7Ou1uZxcJ:scholar.google.com/&output=citation&scisdr=ClFhYC8oEMfL2CygHEY:AFWwaeYAAAAAZoimBEb3WCLW2CA2h-aQ7-HYrwk&scisig=AFWwaeYAAAAAZoimBPRzJgVoEbkgpI97WH85M4M&scisf=4&ct=citation&cd=-1&hl=zh-CN&scfhb=1">bibtex</a>
            </div>
        </div>    
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/aacl.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2302.06072">Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation</a><br/>
                <span class="bold">Bingqian Lin</span>, Yi Zhu, Xiaodan Liang, Liang Lin, Jianzhuang Liu<br/>
                <span class="italic">AAAI Oral</span>, 2023<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2302.06072">arXiv</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:mLWc7Y5goJ0J:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuPFH8:AFWwaeYAAAAAZn-JDH8pu3o5gKGApkq9EsYZexs&scisig=AFWwaeYAAAAAZn-JDO-zi0mqkG5H0L1spOvdEYU&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
            </div>
        </div>
	<div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/RELCLIP.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://aclanthology.org/2022.emnlp-main.317/">RelCLIP: Adapting Language-Image Pretraining for Visual Relationship Detection via Relational Contrastive Learning</a><br/>
                Yi Zhu*, Zhaoqing Zhu*, <span class="bold">Bingqian Lin</span>, Xiaodan Liang, Feng Zhao, Jianzhuang Liu<br/>
                <span class="italic">EMNLP</span>, 2022<br/>
                <a class="btn btn-red" href="https://aclanthology.org/2022.emnlp-main.317/">paper</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:6FC2Wx2Fzn8J:scholar.google.com/&output=citation&scisdr=ClFhYC8oEMfL2CygyZQ:AFWwaeYAAAAAZoim0ZTW1cPSEMP0B-OS9eV5-_E&scisig=AFWwaeYAAAAAZoim0ZTJrpQg0tScG5vbPi9ZHrI&scisf=4&ct=citation&cd=-1&hl=zh-CN&scfhb=1">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/adapt.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2205.15509">ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts</a><br/>
                <span class="bold">Bingqian Lin</span>, Yi Zhu, Zicong Chen, Xiwen Liang, Jianzhuang Liu, Xiaodan Liang<br/>
                <span class="italic">CVPR</span>, 2022<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2205.15509">arXiv</a> / <a class="btn" href="https://github.com/expectorlin/ADAPT">code</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:7VMIlTCalAkJ:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuMaOw:AFWwaeYAAAAAZn-KcOyEVN7eDcrElZOdcgWArL4&scisig=AFWwaeYAAAAAZn-KcB2I1t3uStAuHJQhgRgO9So&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
            </div>
        </div>
	<div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/CITL.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2112.04138">Contrastive Instruction-Trajectory Learning for Vision-Language Navigation</a><br/>
                Xiwen Liang, Fengda Zhu, Yi Zhu, <span class="bold">Bingqian Lin</span>, Bing Wang, Xiaodan Liang<br/>
                <span class="italic">AAAI</span>, 2022<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2112.04138">arXiv</a> / <a class="btn" href="https://github.com/liangcici/CITL-VLN">code</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:53ukXGvhvjoJ:scholar.google.com/&output=citation&scisdr=ClFhYC8oEMfL2CyuFoY:AFWwaeYAAAAAZoioDoY5J1uhrSNAEhccQlkNeLk&scisig=AFWwaeYAAAAAZoioDvBxS93Fn3X9tFoNTFu7qfc&scisf=4&ct=citation&cd=-1&hl=zh-CN&scfhb=1">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/dr-attacker.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2107.11252">Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation</a><br/>
                <span class="bold">Bingqian Lin</span>, Yi Zhu, Yanxin Long, Xiaodan Liang, Qixiang Ye, Liang Lin<br/>
                <span class="italic">TPAMI</span>, 2021<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2107.11252">arXiv</a> / <a class="btn" href="https://github.com/expectorlin/DR-Attacker">code</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:YtupTQ9fM3UJ:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuNNQQ:AFWwaeYAAAAAZn-LLQRN1Y6AhWbndK0D7B1lDn8&scisig=AFWwaeYAAAAAZn-LLSHgZ9phYLkaiZJISD-vK5c&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
            </div>
        </div>
	<div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/CMN.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2003.06745">Vision-Dialog Navigation by Exploring Cross-modal Memory</a><br/>
                Yi Zhu, Fengda Zhu, Zhaohuan Zhan, <span class="bold">Bingqian Lin</span>, Jianbin Jiao, Xiaojun Chang, Xiaodan Liang<br/>
                <span class="italic">CVPR</span>, 2020<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2003.06745">arXiv</a> / <a class="btn" href="https://github.com/yeezhu/CMN.pytorch">code</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:uC_747FIaQkJ:scholar.google.com/&output=citation&scisdr=ClFhYC8oEMfL2CyupU4:AFWwaeYAAAAAZoiovU4QzSa-uw3jOZYpP9ZPfT4&scisig=AFWwaeYAAAAAZoiovZp7_I5aEKjjHPJaZqX2ego&scisf=4&ct=citation&cd=-1&hl=zh-CN&scfhb=1">bibtex</a>
            </div>
        </div>
    </div>
    <div class="section-spacing">
	<div>
	    <h2 class="noselect">Academic Services</h2>
	    <span class="bold">Reviewer for Journal</span><br/>
	    <ul>
		<li class="list-item-spacing" >IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li>
		<li class="list-item-spacing" >IEEE Transactions on Multimedia (TMM)</li>
		<li class="list-item-spacing" >IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
		<li class="list-item-spacing" >IEEE Transactions on Medical Imaging (TMI)</li>
		<li class="list-item-spacing" >Neural Networks</li>
		<!-- Add more list items here if needed -->
	    </ul>
	    <span class="bold">Reviewer for Conference</span><br/>
	    <ul>
		<li class="list-item-spacing" >IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
		<li class="list-item-spacing" >Annual Conference on Neural Information Processing Systems (Neurips)</li>
		<li class="list-item-spacing" >Annual Meeting of the Association for Computational Linguistics (ACL)</li>
		<li class="list-item-spacing" >Empirical Methods in Natural Language Processing (EMNLP)</li>
		<!-- Add more list items here if needed -->
	    </ul>
	</div>
    </div>
    <div class="noselect">
        <a id="contact"></a>
        <h2>Contact</h2>
        You are very welcome to contact me regarding my research. I typically respond within a few days.<br/>
	I can be contacted directly at <span class="bold">bingqianlin</span> [at] <span class="bold">126</span>.com
    </div>
</div>
<div class="footer noselect">
    <div class="footer-content">
        Thanks for the website design from Nicklas Hansen <a href="https://nicklashansen.github.io/">here</a>.
    </div>
</div>
