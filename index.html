<!DOCTYPE html>
<head>
    <title>Bingqian Lin</title>
    <meta name="author" content="Bingqian Lin">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="Bingqian Lin">
	<meta property="og:description" content="PhD student, Sun Yat-sen University">
    <meta property="og:image" content="https://expectorlin.github.io/files/me-cut-new.png">
	<meta property="og:url" content="https://expectorlin.github.io/">
<!-- 	<meta name="twitter:card" content="summary_large_image"> -->
    <link rel="apple-touch-icon" href="files/ucsd-logo.png">
    <link rel="icon" type="image/png" href="files/ucsd-logo.png">
    <link rel="manifest" href="files/site.webmanifest">
    <link rel="stylesheet" href="style.css">
</head>

<div class="header noselect">
    <div class="content row">
        <div class="header-profile-picture"></div>
        <div class="header-text">
            <div class="header-name">
                <h1>Bingqian Lin</h1>
            </div>
            <div class="header-subtitle">
                PhD student, Sun Yat-sen University
            </div>
            <div class="header-links">
                <a class="btn" href="#contact">Email</a> /
                <a class="btn" href="https://scholar.google.com/citations?user=7tNbAJcAAAAJ&hl=zh-CN&view_op=list_works&sortby=pubdate">Google Scholar</a> /
                <a class="btn" href="https://github.com/expectorlin">GitHub</a> 
            </div>
        </div>
    </div>
</div>
<div class="content" style="padding-bottom: 64px;">
    <div>
        <p>
            I am a PhD student at <a href="https://www.sysu.edu.cn/">Sun Yat-sen University</a>, advised by Prof. <a href="https://lemondan.github.io/">Xiaodan Liang</a> and Prof. <a href="http://www.linliang.net/">Liang Lin</a> at the <a href="https://www.sysu-hcp.net/">Human Cyber Physical Intelligence Integration Lab (HCP-I2 Lab)</a>. I was a research intern at Huawei Noahâ€™s Ark Laboratory, advised by Prof. <a href="https://people.ucas.ac.cn/~jzliu?language=en">Jianzhuang Liu</a>.
            I received my BS and MS degrees from the University of Electronic Science and Technology of China (<a href="https://www.uestc.edu.cn/">UESTC</a>) and the Xiamen University (<a href="https://www.xmu.edu.cn/">XMU</a>).<br/><br/>
            My research interest lies in <span class="bold">multi-modal understanding</span>, <span class="bold">robotics</span>, and <span class="bold">computer vision</span>. Currently, I am focusing on building generalist embodied agents driven by foundation models and the world model theory.
        </p>
    </div>
    <div class="section-spacing">
	<div>
	    <h2 class="noselect">Recent Activities</h2>
	    <ul>
		<li class="list-item-spacing" ><span class="bold">[05/2024]</span> &nbsp; One paper on Correctable Landmark Discovery for the VLN task is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE TPAMI</a>.</li>
		<li class="list-item-spacing" ><span class="bold">[05/2024]</span> &nbsp; One paper on Map-oriented Prompting for the VLN task is accepted by <a href="https://2024.aclweb.org/">ACL 2024</a>.</li>
		<li class="list-item-spacing" ><span class="bold">[03/2024]</span> &nbsp; One paper on Navigational Chain-of-Thought for the VLN task is available at <a href="https://arxiv.org/abs/2403.07376">Preprint</a>.</li>
		
		<!-- Add more list items here if needed -->
	    </ul>
	</div>
    </div>
    <div>
        <h2 class="noselect">Selected Publications/Preprints</h2>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/console.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2405.18721">Correctable Landmark Discovery via Large Models for Vision-Language Navigation</a><br/>
                <span class="bold">Bingqian Lin</span>*, Yunshuang Nie*, Ziming Wei, Yi Zhu, Hang Xu, Shikui Ma, Jianzhuang Liu, Xiaodan Liang<br/>
                <span class="italic">TPAMI</span>, 2024<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2405.18721">arXiv</a> / <a class="btn" href="https://github.com/expectorlin/CONSOLE">code</a> / <a class="btn" href="https://scholar.googleusercontent.com/scholar.bib?q=info:aawz8As7-isJ:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuDsYU:AFWwaeYAAAAAZn-FqYWKr6syabehBLI4bnvP60M&scisig=AFWwaeYAAAAAZn-FqdvwYHbg-gzrdzg3XjfXGqY&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a> 
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/navcot.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2403.07376">NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning</a><br/>
                <span class="bold">Bingqian Lin</span>*, Yunshuang Nie*, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang<br/>
                <span class="italic">arXiv</span>, 2024<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2403.07376">arXiv</a> / <a class="btn" href="https://github.com/expectorlin/NavCoT">code</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:4eMBD5Yhe-kJ:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuBElg:AFWwaeYAAAAAZn-HCli5EWPUUsvz76wDLMbWjvw&scisig=AFWwaeYAAAAAZn-HClJR8L5hT8rfLMtHijkmTAw&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/mapgpt.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2401.07314">MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation</a><br/>
                Jiaqi Chen, <span class="bold">Bingqian Lin</span>, Ran Xu, Zhenhua Chai, Xiaodan Liang, Kwan-Yee K. Wong<br/>
                <span class="italic">ACL</span>, 2024<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2401.07314">arXiv</a> / <a class="btn" href="https://github.com/chen-judge/MapGPT">code</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:nQ7JSWn7GecJ:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuOGAk:AFWwaeYAAAAAZn-IAAmazTkgcw8AyPSB6A2lwiw&scisig=AFWwaeYAAAAAZn-IALbSxLp8hJW3jMy4AkGj_xs&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/aacl.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2302.06072">Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation</a><br/>
                <span class="bold">Bingqian Lin</span>, Yi Zhu, Xiaodan Liang, Liang Lin, Jianzhuang Liu<br/>
                <span class="italic">AAAI Oral</span>, 2023<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2302.06072">arXiv</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:mLWc7Y5goJ0J:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuPFH8:AFWwaeYAAAAAZn-JDH8pu3o5gKGApkq9EsYZexs&scisig=AFWwaeYAAAAAZn-JDO-zi0mqkG5H0L1spOvdEYU&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/proper.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2403.05770">Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning</a><br/>
                <span class="bold">Bingqian Lin</span>*, Yanxin Long*, Yi Zhu, Fengda Zhu, Xiaodan Liang, Qixiang Ye, Liang Lin<br/>
                <span class="italic">TPAMI</span>, 2024<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2403.05770">arXiv</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:7nR7E7Fqx44J:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuPn1I:AFWwaeYAAAAAZn-Jh1JrZAYs_IO67HjdtpFQTrE&scisig=AFWwaeYAAAAAZn-Jh3nD5sEa5EtLaAZohzQVY64&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/adapt.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2205.15509">ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts</a><br/>
                <span class="bold">Bingqian Lin</span>, Yi Zhu, Zicong Chen, Xiwen Liang, Jianzhuang Liu, Xiaodan Liang<br/>
                <span class="italic">CVPR</span>, 2022<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2205.15509">arXiv</a> / <a class="btn" href="https://github.com/expectorlin/ADAPT">code</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:7VMIlTCalAkJ:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuMaOw:AFWwaeYAAAAAZn-KcOyEVN7eDcrElZOdcgWArL4&scisig=AFWwaeYAAAAAZn-KcB2I1t3uStAuHJQhgRgO9So&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/dr-attacker.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2107.11252">Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation</a><br/>
                <span class="bold">Bingqian Lin</span>, Yi Zhu, Yanxin Long, Xiaodan Liang, Qixiang Ye, Liang Lin<br/>
                <span class="italic">TPAMI</span>, 2021<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2107.11252">arXiv</a> / <a class="btn" href="https://github.com/expectorlin/DR-Attacker">code</a> / <a class="btn btn-dark" href="https://scholar.googleusercontent.com/scholar.bib?q=info:YtupTQ9fM3UJ:scholar.google.com/&output=citation&scisdr=ClFwuykLEMfL2NuNNQQ:AFWwaeYAAAAAZn-LLQRN1Y6AhWbndK0D7B1lDn8&scisig=AFWwaeYAAAAAZn-LLSHgZ9phYLkaiZJISD-vK5c&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
            </div>
        </div>
    <div class="section-spacing">
	<div>
	    <h2 class="noselect">Academic Services</h2>
	    <span class="bold">Reviewer for Journal</span><br/>
	    <ul>
		<li class="list-item-spacing" >IEEE Transactions on Multimedia (TMM)</li>
		<li class="list-item-spacing" >IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
		<li class="list-item-spacing" >IEEE Transactions on Medical Imaging (TMI)</li>
		<li class="list-item-spacing" >Neural Networks</li>
		<!-- Add more list items here if needed -->
	    </ul>
	    <span class="bold">Reviewer for Conference</span><br/>
	    <ul>
		<li class="list-item-spacing" >IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
		<li class="list-item-spacing" >ACL HomeAssociation for Computational Linguistics (ACL)</li>
		<li class="list-item-spacing" >Annual Conference on Neural Information Processing Systems (Neurips)</li>
		<!-- Add more list items here if needed -->
	    </ul>
	</div>
    </div>
    <div class="noselect">
        <a id="contact"></a>
        <h2>Contact</h2>
        You are very welcome to contact me regarding my research. I can be contacted directly at <span class="bold">bingqianlin</span> [at] <span class="bold">126</span>.com
    </div>
</div>
<div class="footer noselect">
    <div class="footer-content">
       Thanks for the website design from Nicklas Hansen <a href="https://nicklashansen.github.io/">here</a>.
    </div>
</div>
